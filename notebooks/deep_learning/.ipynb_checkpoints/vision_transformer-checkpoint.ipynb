{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f1e23b-5489-4c0b-854b-274e72bc6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5eaab-193a-489f-8a2e-dfc9b09cb472",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c52c7ff-6910-4356-86d4-f54b5357c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"~/prep/data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4e266-bf3b-4328-b7c8-7d0a4eb39b31",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c6ed5a-c80c-424b-832f-613ab99a9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ayush.sharma/prep/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ayush.sharma/prep/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ayush.sharma/prep/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ayush.sharma/prep/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ayush.sharma/prep/data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define transformations (resize to 32x32 as ViT generally takes larger inputs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 for ViT input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize MNIST data: The MNIST dataset has pixel values in the range [0, 1]\n",
    "])\n",
    "\n",
    "## The data will now be in the range [-1, 1]\n",
    "\n",
    "train_dataset = datasets.MNIST(root=root_dir, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=root_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) # No shuffle as test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a10ea8-96ed-4a5f-882a-29cc7ef1ba1a",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc577fc-01fe-4591-a3d1-fe6b977f64f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 32, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1b630-9707-48a3-881b-e58808975832",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30e05223-b7e3-4434-927f-9479e32198ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config():\n",
    "    num_epochs: int = 10\n",
    "    batch_size: int = 64\n",
    "    channels: int = 1\n",
    "    image_size: int = 32\n",
    "    patch_size: int = 16 # should divide image size\n",
    "    num_patches: int = (image_size // patch_size) ** 2\n",
    "    embed_dim: int = patch_size * patch_size\n",
    "    num_transformer_blocks: int = 6 # Paper uses 12 blocks. But since MNIST is small, I use 6\n",
    "    num_heads: int = 4\n",
    "    num_classes: int=10 # 10 classes in MNIST\n",
    "    device: str = \"cuda\" \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929a25-a809-4a2e-bdcd-f349d6ea78b9",
   "metadata": {},
   "source": [
    "## Auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "387b06c3-5e0e-47ab-b836-187704d71d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_image(data: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
    "    batch_size, channels, H, W = data.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0\n",
    "    patches = data.unfold(dimension=2, size=patch_size, step=patch_size).unfold(dimension=3, size=patch_size, step=patch_size)\n",
    "    \n",
    "    patches = patches.contiguous().view(batch_size, -1, patch_size * patch_size * channels)\n",
    "    # Patches has shape [num_batch, num_patch, patch_size * patch_size]. Note seq_len is also called num_patch \n",
    "    return patches\n",
    "    \n",
    "# patches = patchify_image(images, Config.patch_size)\n",
    "# patches.shape\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1872751-d105-49f1-94dc-fbf862a1d3a9",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7771bc-8395-4db0-af02-ef24532a1dd9",
   "metadata": {},
   "source": [
    "### Multi head attention block\n",
    "\n",
    "<img src=\"../assets/mha.png\" alt=\"Multi head attention\"> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7943c429-f837-4a94-b086-206b6f3bff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads: int, input_dim: int, embed_dim: int):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.queries = nn.Linear(input_dim, embed_dim)\n",
    "        self.keys = nn.Linear(input_dim, embed_dim)\n",
    "        self.values = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Now we need to combine the output. A layer for that\n",
    "        self.out = nn.Linear(output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        batch_size, seq_len, input_dim = x.shape   \n",
    "        assert input_dim == self.input_dim # the one provided in cofig and the one in input should match\n",
    "        per_head_dim = self.embed_dim // self.num_heads\n",
    "                            \n",
    "                             \n",
    "        # Projecting the input to find queries, keys and values\n",
    "        Q = self.queries(x)\n",
    "        K = self.keys(x)\n",
    "        V = self.values(x)\n",
    "                                 \n",
    "        # split the Q, K, V across individual heads \n",
    "        # Note that we want the output to be [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        Q = Q.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        K = K.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        V = V.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        \n",
    "        scale = torch.sqrt(torch.tensor(per_head_dim, dtype=torch.float32))\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale # shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by V to get the attention output. \n",
    "        # Note we don't use * as its element wise multiplicaton. We want matmul usch as:\n",
    "        # [batch_size, num_heads, seq_len, seq_len] <dot> [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        attention_output = torch.matmul(attention_weights, V) # shape: [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        \n",
    "        attention_output_og_shape = attention_output.transpose(1,2).contiguous() # transpose makes a tensor not-contiguous. So , make a new contigous copy\n",
    "        attention_output_og_shape = attention_output_og_shape.view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Note that we cold have also done it using reshape\n",
    "        # attention_output_og_shape = attention_output.transpose(1, 2).reshape(x.size(0), x.size(1), self.embed_dim)\n",
    "        \n",
    "        return attention_output_og_shape\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16355f7c-8550-4721-81a2-f94918f9d1ee",
   "metadata": {},
   "source": [
    "### Questions <br>\n",
    "\n",
    "\n",
    "#### 1. What is sequence length here?\n",
    "Answer: THe sequence length here is the number of patches of the image. The ViT authors make the analogy being each grid is a word and `[patch_size x patch_size` is the dimension of the embedding of that word.\n",
    "\n",
    "#### 2. Why do we use transpose in this part of the code? <br>\n",
    "```\n",
    "Q = Q.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "K = K.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "V = V.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "```\n",
    "\n",
    "Answer:\n",
    " - After reshaping the tensor into [batch_size, seq_len, num_heads, head_dim], we need to change the order of the dimensions so that the num_heads dimension comes before the sequence length (seq_len). This is necessary because we want to compute attention independently for each head.\n",
    " - By transposing, we rearrange the tensor into:\n",
    "   `[batch_size, num_heads, seq_len, per_head_dim]`\n",
    "   we can now apply attention on the last 2 dimensions which is `sequence_length`, `per_head_dim`\n",
    "\n",
    "\n",
    "#### 3. (Important) Given word embeddings of dimension 128 for a sequence of 10 words (so, shape [batch_size, 10, 128]), if we linearly project these into queries of shape [batch_size, 10, 128], and then apply multi-head attention:\n",
    " - Why does multi-head attention split the word embedding into chunks?\n",
    " - **Doesn't this splitting lose the meaning of the word embedding**?\n",
    " \n",
    "Answer: <br>\n",
    "- Let's take an input example <br>\n",
    "    Input Example: Word embedding has shape [batch_size, 10, 128], where:\n",
    "    - 10 is the number of words in the sequence.\n",
    "    - 128 is the word embedding dimension.\n",
    "\n",
    "- Linear Projection:\n",
    "  Each word embedding (128-dimensional) is linearly projected into queries, keys, and values before splitting into heads.\n",
    "  \n",
    "- Splitting Across Heads:\n",
    "  After projection, multi-head attention splits the transformed representations, not the raw word embeddings.\n",
    "  For 8 heads, the projected queries are split into 8 parts of size 16 (128 / 8 = 16).\n",
    "  \n",
    "- The linear projections allow each head to attend to a **different transformed representation of the word embedding**, enabling multiple diverse attention patterns.\n",
    "\n",
    "- Each head focuses on a portion of the transformed embedding, but not directly splitting the original embedding.\n",
    "\n",
    "- The core reason we don’t directly project into a [batch_size, seq_len, num_heads, head_dim] space (where num_heads × head_dim = input_dim) is **primarily for flexibility and efficient parameter sharing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be687d1f-c982-4370-b085-889fdd61f7f5",
   "metadata": {},
   "source": [
    "<img src=\"../assets/transformer_model_architecture.png\" alt=\"ViT architecture\"> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e558a08-b977-45a8-820f-05633fe6e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads: int, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.norm1 = nn.LayerNorm(input_dim) ## Normalize before attention\n",
    "        self.mha = SelfAttentionBlock(num_heads, input_dim, output_dim)\n",
    "        self.norm2 = nn.LayerNorm(output_dim) # layer norm after the transformer MHA block\n",
    "        self.ffn = nn.Sequential(\n",
    "                      nn.Linear(output_dim, output_dim * 4), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(output_dim * 4, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        assert input_dim == self.input_dim, \"The shapes of input dim specified in constructor and the data are not same\"\n",
    "        x = self.norm1(x)\n",
    "        # The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 2\n",
    "        attn_output = self.mha(x)\n",
    "        print(\"atoutput\", attn_output.shape)\n",
    "        x = attn_output + x     # Residual connection\n",
    "        x = self.norm2(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        output = ffn_output + x # Residual connection\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b41acc04-d82b-4474-b56f-4dc5c573702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(num_epochs=10, batch_size=64, channels=1, image_size=32, patch_size=16, num_patches=4, embed_dim=256, output_dim=64, num_transformer_blocks=6, num_heads=4, num_classes=10, device='cuda')\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56b5b363-dcf7-4e6f-b3a9-654de963e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        super(ViT, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        num_patches = cfg.num_patches\n",
    "        embed_dim = cfg.embed_dim\n",
    "        input_dim = cfg.patch_size ** 2\n",
    "        \n",
    "        self.input_projection = None\n",
    "        if input_dim != embed_dim:\n",
    "            self.input_projection = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # parameters for positional embedding\n",
    "        # we use a learable positional embed as used in ViT paper\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.transformers = nn.Sequential(\n",
    "                                *[TransformerBlock(cfg.num_heads, cfg.embed_dim, cfg.output_dim) for _ in range(cfg.num_transformer_blocks)]\n",
    "                            )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(cfg.output_dim, cfg.num_classes)  # For 10 MNIST classes\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        patches = patchify_image(x, self.cfg.patch_size)\n",
    "        assert patches.shape[1] == self.cfg.num_patches, \\\n",
    "            f\"Number of patches {patches.shape[1]} should match {self.cfg.num_patches}\"\n",
    "        if self.input_projection:\n",
    "            patches = self.input_projection(patches)\n",
    "        patches = patches + self.pos_embed # [batch_size,  seq_len, embed_dim] Note embed_dim is the input_dim\n",
    "        x = patches.contiguous()\n",
    "        x = self.transformers(x) # [batch_size,  seq_len, output_dim]\n",
    "        x = self.global_avg_pool(x) # Apply adaptive pooling to get [batch_size, output_dim, 1]\n",
    "        x = x.squeeze(-1)\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9979c65-54e3-4860-abac-93fe36378c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e4136ec-d9cd-45c7-865f-14702bb9b7e0",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5d6301c-5a4e-4020-b0df-86a1310562b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atoutput torch.Size([64, 4, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \n\u001b[1;32m     34\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg, model, train_loader, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(images.shape, labels.shape)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/prep/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[61], line 27\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;66;03m# [batch_size,  seq_len, embed_dim] Note embed_dim is the input_dim\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [batch_size,  seq_len, output_dim]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_avg_pool(x) \u001b[38;5;66;03m# Apply adaptive pooling to get [batch_size, output_dim, 1]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/prep/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/prep/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/prep/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[65], line 24\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matoutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m     \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n\u001b[1;32m     26\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def train(cfg: Config,\n",
    "          model: nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          criterion, \n",
    "         ):\n",
    "    num_epochs = cfg.num_epochs\n",
    "    device = cfg.device\n",
    "    model.to(device)\n",
    "    running_loss = 0\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # print(images.shape, labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        opimizer.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        print(outputs.shape, labels.shape)\n",
    "        break\n",
    "        \n",
    "\n",
    "cfg = Config()\n",
    "model = ViT(cfg)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "train(cfg, model, train_loader, criterion)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45250c42-3724-4a8a-810e-f9094f0584d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
