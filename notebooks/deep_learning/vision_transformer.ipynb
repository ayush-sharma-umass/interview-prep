{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f1e23b-5489-4c0b-854b-274e72bc6601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush.sharma/miniconda3/envs/prep/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ayush.sharma/miniconda3/envs/prep/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os, os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5eaab-193a-489f-8a2e-dfc9b09cb472",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c52c7ff-6910-4356-86d4-f54b5357c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"~/prep/data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4e266-bf3b-4328-b7c8-7d0a4eb39b31",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c6ed5a-c80c-424b-832f-613ab99a9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (resize to 32x32 as ViT generally takes larger inputs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 for ViT input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize MNIST data: The MNIST dataset has pixel values in the range [0, 1]\n",
    "])\n",
    "\n",
    "## The data will now be in the range [-1, 1]\n",
    "\n",
    "train_dataset = datasets.MNIST(root=root_dir, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=root_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False) # No shuffle as test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a10ea8-96ed-4a5f-882a-29cc7ef1ba1a",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc577fc-01fe-4591-a3d1-fe6b977f64f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 32, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1b630-9707-48a3-881b-e58808975832",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e05223-b7e3-4434-927f-9479e32198ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config():\n",
    "    num_epochs: int = 10\n",
    "    batch_size: int = 64\n",
    "    channels: int = 1\n",
    "    image_size: int = 32\n",
    "    patch_size: int = 16 # should divide image size\n",
    "    num_patches: int = (image_size // patch_size) ** 2\n",
    "    embed_dim: int = patch_size * patch_size\n",
    "    num_transformer_blocks: int = 6 # Paper uses 12 blocks. But since MNIST is small, I use 6\n",
    "    num_heads: int = 4\n",
    "    num_classes: int=10 # 10 classes in MNIST\n",
    "    device: str = \"cuda\" \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929a25-a809-4a2e-bdcd-f349d6ea78b9",
   "metadata": {},
   "source": [
    "## Auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387b06c3-5e0e-47ab-b836-187704d71d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_image(data: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
    "    batch_size, channels, H, W = data.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0\n",
    "    patches = data.unfold(dimension=2, size=patch_size, step=patch_size).unfold(dimension=3, size=patch_size, step=patch_size)\n",
    "    \n",
    "    patches = patches.contiguous().view(batch_size, -1, patch_size * patch_size * channels)\n",
    "    # Patches has shape [num_batch, num_patch, patch_size * patch_size]. Note seq_len is also called num_patch \n",
    "    return patches\n",
    "    \n",
    "# patches = patchify_image(images, Config.patch_size)\n",
    "# patches.shape\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1872751-d105-49f1-94dc-fbf862a1d3a9",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7771bc-8395-4db0-af02-ef24532a1dd9",
   "metadata": {},
   "source": [
    "### Multi head attention block\n",
    "\n",
    "<img src=\"../assets/mha.png\" alt=\"Multi head attention\"> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7943c429-f837-4a94-b086-206b6f3bff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads: int, input_dim: int, embed_dim: int):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.queries = nn.Linear(input_dim, embed_dim)\n",
    "        self.keys = nn.Linear(input_dim, embed_dim)\n",
    "        self.values = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Now we need to combine the output. A layer for that\n",
    "        self.out = nn.Linear(embed_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        batch_size, seq_len, input_dim = x.shape   \n",
    "        assert input_dim == self.input_dim # the one provided in cofig and the one in input should match\n",
    "        per_head_dim = self.embed_dim // self.num_heads\n",
    "                            \n",
    "                             \n",
    "        # Projecting the input to find queries, keys and values\n",
    "        Q = self.queries(x)\n",
    "        K = self.keys(x)\n",
    "        V = self.values(x)\n",
    "                                 \n",
    "        # split the Q, K, V across individual heads \n",
    "        # Note that we want the output to be [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        Q = Q.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        K = K.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        V = V.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "        \n",
    "        scale = torch.sqrt(torch.tensor(per_head_dim, dtype=torch.float32))\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale # shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by V to get the attention output. \n",
    "        # Note we don't use * as its element wise multiplicaton. We want matmul usch as:\n",
    "        # [batch_size, num_heads, seq_len, seq_len] <dot> [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        attention_output = torch.matmul(attention_weights, V) # shape: [batch_size, num_heads, seq_len, per_head_dim]\n",
    "        \n",
    "        attention_output_og_shape = attention_output.transpose(1,2).contiguous() # transpose makes a tensor not-contiguous. So , make a new contigous copy\n",
    "        attention_output_og_shape = attention_output_og_shape.view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Note that we cold have also done it using reshape\n",
    "        # attention_output_og_shape = attention_output.transpose(1, 2).reshape(x.size(0), x.size(1), self.embed_dim)\n",
    "        \n",
    "        output = self.out(attention_output_og_shape)\n",
    "        return output\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16355f7c-8550-4721-81a2-f94918f9d1ee",
   "metadata": {},
   "source": [
    "### Questions <br>\n",
    "\n",
    "\n",
    "#### 1. What is sequence length here?\n",
    "Answer: THe sequence length here is the number of patches of the image. The ViT authors make the analogy being each grid is a word and `[patch_size x patch_size` is the dimension of the embedding of that word.\n",
    "\n",
    "#### 2. Why do we use transpose in this part of the code? <br>\n",
    "```\n",
    "Q = Q.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "K = K.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "V = V.view(x.size(0), x.size(1), self.num_heads, per_head_dim).transpose(1, 2)\n",
    "```\n",
    "\n",
    "Answer:\n",
    " - After reshaping the tensor into [batch_size, seq_len, num_heads, head_dim], we need to change the order of the dimensions so that the num_heads dimension comes before the sequence length (seq_len). This is necessary because we want to compute attention independently for each head.\n",
    " - By transposing, we rearrange the tensor into:\n",
    "   `[batch_size, num_heads, seq_len, per_head_dim]`\n",
    "   we can now apply attention on the last 2 dimensions which is `sequence_length`, `per_head_dim`\n",
    "\n",
    "\n",
    "#### 3. (Important) Given word embeddings of dimension 128 for a sequence of 10 words (so, shape [batch_size, 10, 128]), if we linearly project these into queries of shape [batch_size, 10, 128], and then apply multi-head attention:\n",
    " - Why does multi-head attention split the word embedding into chunks?\n",
    " - **Doesn't this splitting lose the meaning of the word embedding**?\n",
    " \n",
    "Answer: <br>\n",
    "- Let's take an input example <br>\n",
    "    Input Example: Word embedding has shape [batch_size, 10, 128], where:\n",
    "    - 10 is the number of words in the sequence.\n",
    "    - 128 is the word embedding dimension.\n",
    "\n",
    "- Linear Projection:\n",
    "  Each word embedding (128-dimensional) is linearly projected into queries, keys, and values before splitting into heads.\n",
    "  \n",
    "- Splitting Across Heads:\n",
    "  After projection, multi-head attention splits the transformed representations, not the raw word embeddings.\n",
    "  For 8 heads, the projected queries are split into 8 parts of size 16 (128 / 8 = 16).\n",
    "  \n",
    "- The linear projections allow each head to attend to a **different transformed representation of the word embedding**, enabling multiple diverse attention patterns.\n",
    "\n",
    "- Each head focuses on a portion of the transformed embedding, but not directly splitting the original embedding.\n",
    "\n",
    "- The core reason we don’t directly project into a [batch_size, seq_len, num_heads, head_dim] space (where num_heads × head_dim = input_dim) is **primarily for flexibility and efficient parameter sharing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be687d1f-c982-4370-b085-889fdd61f7f5",
   "metadata": {},
   "source": [
    "<img src=\"../assets/transformer_model_architecture.png\" alt=\"ViT architecture\"> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e558a08-b977-45a8-820f-05633fe6e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads: int, \n",
    "                 input_dim: int, \n",
    "                 embed_dim: int):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm1 = nn.LayerNorm(input_dim) ## Normalize before attention\n",
    "        self.mha = SelfAttentionBlock(num_heads, input_dim, embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim) # layer norm after the transformer MHA block\n",
    "        self.ffn = nn.Sequential(\n",
    "                      nn.Linear(embed_dim, embed_dim * 4), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        assert input_dim == self.input_dim, \"The shapes of input dim specified in constructor and the data are not same\"\n",
    "\n",
    "        # Multi-head attention block\n",
    "        attn_output = self.mha(self.norm1(x))  # Apply LayerNorm first\n",
    "        x = x + attn_output  # Residual connection after attention\n",
    "\n",
    "        # Feed-forward network block\n",
    "        ffn_output = self.ffn(self.norm2(x))  # Apply LayerNorm first\n",
    "        output = x + ffn_output  # Residual connection after feed-forward\n",
    "        return output\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6790b-3ca6-4915-b5a9-6237ed2e72bc",
   "metadata": {},
   "source": [
    "### 4. Why do we use LayerNorm in Transformers?\n",
    "\n",
    "Answer: There are multiple benefits to using LayerNorm in Transformer. The main reason we use it with Transformer, is that because transformers attend to individual tokens independently. So taking norm across the layer dimensions gives a more representative output as each token in `[batch, seq_len, embed_dim]` has its own feature space of `[embed_dim]`\n",
    "- LayerNorm is especially useful for transformers since each token is processed individually in attention mechanisms, and LayerNorm ensures that the representation of each token is properly normalized, leading to more effective training.\n",
    "- LayerNorm helps stabilize the training process by normalizing the inputs across the feature dimensions (i.e. normalizing with each token) and ensuring they have zeron mean and variance. This tackles the problem of exploding or vanishing gradients. \n",
    "- It helps stabilize residual connections by preventing large values from accumulating when residuals are added, ensuring smooth information flow through the network.\n",
    "\n",
    "\n",
    "### 5. Why don't we apply ReLU after second linear layer?\n",
    "```\n",
    "self.ffn = nn.Sequential(\n",
    "                      nn.Linear(embed_dim, embed_dim * 4), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "```\n",
    "- **Loss of information before sum**: The final output from the FFN (after the second linear layer) is typically added to the residual connection, which expects the full range of values (both positive and negative) to maintain flexibility in representation. Applying ReLU after the second layer would clip all negative values, potentially losing information.\n",
    "- **Redundant Non Linearity**: \n",
    "    - The purpose of the ReLU (or GELU) activation function is to introduce non-linearity to the model, which helps it learn more complex patterns. After the first linear layer, the ReLU adds this non-linearity.\n",
    "    - Applying another ReLU after the second linear layer would reduce flexibility because the second linear layer's purpose is to project back to the original feature space. Keeping it linear allows the output to maintain the structure needed for the next layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41acc04-d82b-4474-b56f-4dc5c573702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(num_epochs=10, batch_size=64, channels=1, image_size=32, patch_size=16, num_patches=4, embed_dim=256, num_transformer_blocks=6, num_heads=4, num_classes=10, device='cuda')\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f1c86-1cad-44ba-8108-cb713c801888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56b5b363-dcf7-4e6f-b3a9-654de963e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        super(ViT, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        num_patches = cfg.num_patches\n",
    "        embed_dim = cfg.embed_dim\n",
    "        input_dim = cfg.patch_size ** 2\n",
    "        \n",
    "        \n",
    "        # parameters for positional embedding\n",
    "        # we use a learable positional embed as used in ViT paper\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, input_dim))\n",
    "        self.transformers = nn.Sequential(\n",
    "                                *[TransformerBlock(cfg.num_heads, cfg.patch_size ** 2, cfg.embed_dim) for _ in range(cfg.num_transformer_blocks)]\n",
    "                            )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(cfg.patch_size ** 2, cfg.num_classes)  # For 10 MNIST classes\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        patches = patchify_image(x, self.cfg.patch_size)\n",
    "        assert patches.shape[1] == self.cfg.num_patches, \\\n",
    "            f\"Number of patches {patches.shape[1]} should match {self.cfg.num_patches}\"\n",
    "       \n",
    "        # At this point patches shape: [batch_size,  seq_len, input_dim]\n",
    "        patches = patches + self.pos_embed  \n",
    "        x = patches.contiguous()\n",
    "        x = self.transformers(x) # [batch_size,  seq_len, embed_dim]\n",
    "        \n",
    "        # Transpose for AdaptiveAvgPool1d to pool over the sequence length\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, embed_dim, seq_len]\n",
    "        x = self.global_avg_pool(x) # Apply adaptive pooling to get [batch_size, embed_dim, 1]\n",
    "        x = x.squeeze(-1)\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9979c65-54e3-4860-abac-93fe36378c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e4136ec-d9cd-45c7-865f-14702bb9b7e0",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d6301c-5a4e-4020-b0df-86a1310562b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4a06579c064a38a257c43888d904c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Training Accuracy: 92.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b583564500ee4a87a628f5294080d28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Training Accuracy: 96.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26fd24a064543e3ac0fb405a0978ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Training Accuracy: 97.63%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22cbd3ba4a84d92ba40c8c90628148c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Training Accuracy: 97.88%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13c9daf0b4741cda7a4809af1954cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Training Accuracy: 98.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b320c3826bc4f3c9d73bb3c9fc0f794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Training Accuracy: 98.38%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06dfa84bc22494d83a2c191823ddd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Training Accuracy: 98.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a302736da543c8a63a820b697924d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished. Training Accuracy: 98.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09332b04df24a6db1065891ff67765a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished. Training Accuracy: 98.73%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c691d0d2ec407d91ab1d5277ed0838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished. Training Accuracy: 98.66%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Lab (From stack Overflow)\n",
    "\n",
    "\n",
    "\n",
    "def train(cfg: Config,\n",
    "          model: nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          criterion, \n",
    "         ):\n",
    "    num_epochs = cfg.num_epochs\n",
    "    device = cfg.device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "        for idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # print(images.shape, labels.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': running_loss / (idx + 1), \n",
    "                'Accuracy': 100 * correct / total\n",
    "            })\n",
    "\n",
    "        print(f'Epoch {epoch+1} finished. Training Accuracy: {100 * correct / total:.2f}%')\n",
    "    print(\"Finished Training\")            \n",
    "        \n",
    "\n",
    "cfg = Config()\n",
    "model = ViT(cfg)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "train(cfg, model, train_loader, criterion)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45250c42-3724-4a8a-810e-f9094f0584d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "913cf8b3-b902-4e56-aa59-f12746e18daf",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f75b781-9f9e-4153-9e39-50932b1d84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    device = cfg.device\n",
    "    model.to(device)  \n",
    "    model.eval()  # Set model to evaluation mode. Don't forget this.\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during testing\n",
    "        for images, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for precision/recall\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate precision and recall for the test set\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "    return avg_loss, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9733f995-5a94-49b6-a203-fdf3859c4270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3672c603b4954d80b439c1eee1f8838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0859, Precision: 0.9760, Recall: 0.9742\n"
     ]
    }
   ],
   "source": [
    "avg_test_loss, precision, recall = test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c9d87-205d-4af7-b5aa-82d294062844",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f8863ca-8e56-4aee-bc6b-4308be5d0424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " from torchvision.utils import make_grid\n",
    "\n",
    "def visualize_results(cfg, model, test_loader,num_images=8):\n",
    "    device = cfg.device\n",
    "    classes = [str(i) for i in range(cfg.num_classes)]  # For MNIST, class labels are 0-9\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Move the images and labels to the appropriate device (GPU or CPU)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Move the images and labels back to the CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = preds.cpu()\n",
    "    \n",
    "    # Plot the images with predicted and true labels\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].squeeze().numpy()  # Convert to NumPy array and remove single channel\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Pred: {classes[preds[i]]}\\nTrue: {classes[labels[i]]}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_results(cfg, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04153b3d-f658-41b0-b4db-8752a050a8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
