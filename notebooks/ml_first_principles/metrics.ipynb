{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081ac83b-e310-4cf1-961d-bf8cc6b4bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c2286-7e07-460a-9abe-408ccab1e885",
   "metadata": {},
   "source": [
    "# 1. Precision Recall\n",
    "\n",
    "\n",
    "### Overview: \n",
    "\n",
    "In a binary decision problem, a classifier labels examples as either positive or negative. The decision made by the classifier can be represented in a structure known as a confusion matrix or contingency table. The confusion matrix has four categories: \n",
    "- True positives (TP) are examples correctly labeled as positives. \n",
    "- False positives (FP) refer to negative examples incorrectly labeled as positive. \n",
    "- True negatives (TN) correspond to negatives correctly labeled as negative.\n",
    "- False negatives (FN) refer to positive examples incorrectly labeled as negative.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "### Intuitive example: \n",
    "\n",
    "\n",
    "### What do we need to implement for binary classification\n",
    "\n",
    "- A $Y_T$ (GT) array of your true labels\n",
    "- A $Y_P$ (pred) array of our probabilities\n",
    "- $Th$ - a threshold for thresholding leading class (class-0) \n",
    " \n",
    "or \n",
    "\n",
    "- A $Y_T$ (GT) array of your true labels\n",
    "- A $Y_P$ (pred) array of our predicted labels\n",
    "\n",
    "\n",
    "### What does a high precision mean?\n",
    "\n",
    "- It means our model is good at not falsely giving any sample a true label.\n",
    "\n",
    "### What does a high recall mean?\n",
    "\n",
    "- It means our model is good at **not missing out** on good samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20d3271-8ce5-4963-a8b3-73e09091ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.667\n",
      "Recall: 0.800\n"
     ]
    }
   ],
   "source": [
    "##########  Implementation  ###################\n",
    "\n",
    "# True labels (binary: 0 for negative class, 1 for positive class)\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "\n",
    "# Predicted labels (binary: 0 for negative class, 1 for positive class)\n",
    "y_pred = np.array([1, 0, 1, 0, 1, 1, 1, 0, 1, 0])\n",
    "\n",
    "def precision_recall_binary(y_true, y_pred):\n",
    "    \"\"\"Calculate precision and recall for binary classification.\"\"\"\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Recall = TP / (TP + FN)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall = precision_recall_binary(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07e722-f9eb-4687-b0a5-3d8dab579ddf",
   "metadata": {},
   "source": [
    "# 2. Multi class Classification (Precision-Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48eefa-fde8-4a2b-95c3-34ee5a2265df",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "For multiclass classification, where there are more than two classes, precision and recall can be calculated in two main ways:\n",
    "\n",
    "#### Micro-Averaging:\n",
    "\n",
    "- Treats all classes as a single combined class.\n",
    "- Calculates precision and recall globally by counting the total true positives, false positives, and false negatives across all classes.\n",
    "\n",
    "**Micro-averaged Precision**:\n",
    "\n",
    "$$\n",
    "\\text{Micro Precision} = \\frac{\\sum \\text{True Positives}}{\\sum (\\text{True Positives} + \\text{False Positives})}\n",
    "$$\n",
    "\n",
    "**Micro-averaged Recall**:\n",
    "\n",
    "$$\n",
    "\\text{Micro Recall} = \\frac{\\sum \\text{True Positives}}{\\sum (\\text{True Positives} + \\text{False Negatives})}\n",
    "$$\n",
    "\n",
    "#### Macro-Averaging:\n",
    "\n",
    "- Calculates precision and recall for each class separately and then takes the average across all classes.\n",
    "\n",
    "**Macro-averaged Precision**:\n",
    "\n",
    "$$\n",
    "\\text{Macro Precision} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Precision}_i\n",
    "$$\n",
    "\n",
    "**Macro-averaged Recall**:\n",
    "\n",
    "$$\n",
    "\\text{Macro Recall} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Recall}_i\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of classes.\n",
    "\n",
    "#### Weighted Averaging:\n",
    "\n",
    "- Similar to macro-averaging, but weights the precision and recall for each class based on the number of samples in that class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81654ae3-eb3d-4c3a-9e3d-21baf0576365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2 1 0]\n",
      " [0 3 1]\n",
      " [1 0 2]]\n",
      "Per-class Precision: [0.66666667 0.75       0.66666667]\n",
      "Per-class Recall: [0.66666667 0.75       0.66666667]\n",
      "Macro Precision: 0.694\n",
      "Macro Recall: 0.694\n",
      "Micro Precision: 0.700\n",
      "Micro Recall: 0.700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# True labels for 10 samples (multiclass problem with 3 classes: 0, 1, 2)\n",
    "y_true = np.array([0, 1, 2, 1, 0, 2, 1, 0, 1, 2])\n",
    "\n",
    "# Predicted labels for the same 10 samples\n",
    "y_pred = np.array([0, 2, 2, 1, 0, 0, 1, 1, 1, 2])\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 3\n",
    "\n",
    "# Create confusion matrix\n",
    "def confusion_matrix_multiclass(y_true, y_pred, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        matrix[t, p] += 1\n",
    "    return matrix\n",
    "\n",
    "conf_matrix = confusion_matrix_multiclass(y_true, y_pred, num_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Precision and recall calculation for each class\n",
    "def precision_recall_per_class(conf_matrix):\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        \n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# Calculate per-class precision and recall\n",
    "precision, recall = precision_recall_per_class(conf_matrix)\n",
    "print(\"Per-class Precision:\", precision)\n",
    "print(\"Per-class Recall:\", recall)\n",
    "\n",
    "# Macro Averaging: Average precision and recall across all classes\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "\n",
    "print(f\"Macro Precision: {macro_precision:.3f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.3f}\")\n",
    "\n",
    "# Micro Averaging: Total true positives, false positives, and false negatives across all classes\n",
    "def micro_precision_recall(conf_matrix):\n",
    "    \"\"\"Calculate micro-averaged precision and recall.\"\"\"\n",
    "    tp = np.sum(np.diag(conf_matrix))  # sum of true positives for all classes\n",
    "    \n",
    "    # False positives for all classes (sum of columns minus true positives)\n",
    "    fp = np.sum(conf_matrix, axis=0) - np.diag(conf_matrix)\n",
    "    \n",
    "    # False negatives for all classes (sum of rows minus true positives)\n",
    "    fn = np.sum(conf_matrix, axis=1) - np.diag(conf_matrix)\n",
    "    \n",
    "    if (tp + np.sum(fp)) > 0:\n",
    "        micro_precision = tp / (tp + np.sum(fp))\n",
    "    else:\n",
    "        micro_precision = 0\n",
    "    \n",
    "    if (tp + np.sum(fn)) > 0:\n",
    "        micro_recall = tp / (tp + np.sum(fn))\n",
    "    else:\n",
    "        micro_recall = 0\n",
    "    \n",
    "    return micro_precision, micro_recall\n",
    "\n",
    "# Calculate micro-averaged precision and recall\n",
    "micro_precision, micro_recall = micro_precision_recall(conf_matrix)\n",
    "\n",
    "print(f\"Micro Precision: {micro_precision:.3f}\")\n",
    "print(f\"Micro Recall: {micro_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e6e36-8cb4-4d3f-98c4-548ad8d169f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
