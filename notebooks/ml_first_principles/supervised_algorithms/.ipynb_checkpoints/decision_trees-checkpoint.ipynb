{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import treelib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bec108-60c5-4253-9b0d-70f36955688f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of contents:\n",
    "* [What are Decision trees](#1)\n",
    "* [Overview of the algorithm](#2)\n",
    "* [Decision Trees Superpowers](#3)\n",
    "* [Decision Trees are Effective when](#4)\n",
    "* [When to watch out for?](#5)\n",
    "* [Code](#6)\n",
    "* [Node splitting metrics](#7)\n",
    "* [Decision tree class](#8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e69d7-12bf-4884-8e18-cb502084fe76",
   "metadata": {},
   "source": [
    "# What are Decision trees? <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "Decision trees are a popular supervised learning algorithm used for **classification** and **regression** tasks. <br>\n",
    "Although, relatively less asked in interviews, it is one of the most versatile method when it comes to application. <br>\n",
    "It is like a swiss knife of tools and its derivatives such as random forests in combination with techniques like bagging, boosting etc can make it really effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efe242-652a-4c01-88c8-d59408a87d91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Overview of the algorithm <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "1. Data preprocessing: <br>\n",
    "    In general we **don't need to normalize** data, because decision tree is invariant to scale. However, if your data is high dimensional, using PCA to remove un-neccesary features may be a good idea. <br>\n",
    "    \n",
    "2. Find the best node to split: <br>\n",
    "    This is the greedy part. Suppose you had a metric to measure if your data is more or less structured at a split level. Let's call this metric $M$. If M is high, there is more randomness, less structure. <br>\n",
    "    \n",
    "    *We start from the root node, where we have all the data* <br>\n",
    "    $M_P =   M(root) $ which is the randomness in the root level\n",
    "    Now, we do the following:\n",
    "    * for each feature in data $F$:\n",
    "        * for each value in given data of that feature $V$:\n",
    "            We split the the data into two parts: <br>\n",
    "                Left part: All the samples where $F_i <= V$ and, <br>\n",
    "                Right part: All the samples where $F_i > V$ and, <br>\n",
    "                $M_L$ is metric of randomness for left and $M_R$ is for right child, <br>\n",
    "                and we are interested in this quantity: <br>\n",
    "                \n",
    "                $Gain = M_P - (M_L + M_R)$ <br> <br>\n",
    "                \n",
    "    \n",
    "    *Save the feature $F$ and the value of that feature $V$ which gives you the maximum Gain.*: This means the current split, brings the most order in the dataset. <br>\n",
    "    If the difference is high, it means we went from high disorder to more structure.\n",
    "    \n",
    "3. Keep doing step 2 recursively for all the left and right nodes thus created in the process till one of two happens:\n",
    "    *  Data in the split has all the samples of one class, or <br>\n",
    "    *  We have reached the max-depth in the tree (a user-defined hyperparam)\n",
    "    \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed18b0-547b-43d3-8099-4f3732c2d794",
   "metadata": {},
   "source": [
    "# Decision Trees Superpowers <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "Here are some Decision Trees superpowers: <br>\n",
    "\n",
    " *  *Decision Trees don't make assumptions about the underlying data distribution:* <br>\n",
    "     They can handle linear as well as non linear data. Thsi property is also called non-parametric property of Decision trees.\n",
    " *  *Scale invariant*: <br>\n",
    "     Decision trees are invariant to the scale of the data. So, we don't have to normalize data.\n",
    " *  *Mixed data handling*: <br>\n",
    "     Decision trees can handle both categorical and numerical features. They can split the data based on categorical variables and use numerical thresholds for continuous variables.\n",
    " *  *Robust to Outliers*:\n",
    "     Since, the decision tree algorithms work by creating binary splits, any outlier case will likely form a split with only itself in it and will not impact the overall performance.\n",
    " *  *Can understand Feature importance*: <br>\n",
    "     By examining how frequently a feature is used for splitting, we can determine its relative importance in the decision-making process. This is a very distiguishing aspect of Decision Trees.\n",
    " *  *Interpretable*: <br>\n",
    "     Decision trees splits can be visualized and explained unlike other algorithms where explainability is an issue (ex. Neural networks).\n",
    " *  *Handles Missing data*: <br>\n",
    "     Decision trees can handle missing values by using surrogate splits. Surrogate splits allow decision trees to consider alternative features in case a specific feature has missing data.\n",
    "     \n",
    "     \n",
    "     \n",
    "What all this means is Decision trees are very versatile. <br>\n",
    "If you're asked a simple algorithms which can classify a 1-D data where you don't know the underlying distribution, Decision Trees are a very efficient solution. <br>\n",
    "\n",
    "\n",
    "# Decision Trees are Effective when: <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    " *  Features have non-linear relationships.\n",
    " *  Features have interactions with certain other featues.\n",
    " *  Most importantly, if the dataset is such that there can be discrete decisino boundaries, decision trees are extremely effective.\n",
    "\n",
    "     \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9a5b9-c9e3-44be-9d7b-c05ef84d6512",
   "metadata": {},
   "source": [
    "# When to watch out for? <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    " *  *High Dimensional data:* <br>\n",
    "     Whenever there are high number of features, there is a requirement for higher samples as well to understand complex decision boundaries. <br>\n",
    "     In such a case, decision tree may not be the best solution. As a rough guideline, having at least 5 to 10 times as many samples as features is often considered a reasonable starting point. \n",
    " *  *Prone to overfitting if not enough samples:* <br>\n",
    "     Since decision tree is a greedy splitting algorithm, in absence of data, it will overfit to however much data it has, leading to poor generalization. \n",
    " *  *Data with complex dependencies:* <br>\n",
    "     When features have complex non linear dependencies, a decision may not yield good results. Say, more than 3 features have spectral relationships. In such a case, we need to use other techniques such as random forest, gradient boosting.\n",
    " *  *Imbalanced classes:* <br>\n",
    "     Just like many other algorithms (like logistic regression etc), we need to be careful about class imbalance. However, this issue can be mitigated if we're careful. We can use class weighting or resampling.\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef86cab-8ee5-4206-9f8a-7e5f7006b51c",
   "metadata": {},
   "source": [
    "# Coding it <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda1455",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2aad59a-a974-4d47-abd3-6339fbf1fd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootpath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f95c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 150\n",
      "Total features: 5\n",
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n"
     ]
    }
   ],
   "source": [
    "csvpath = os.path.join(rootpath, \"assets/iris.csv\")\n",
    "df = pd.read_csv(csvpath)\n",
    "num_samples, num_feats = df.shape\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "print(f\"Total features: {num_feats}\")\n",
    "feat_names = list(df)\n",
    "print(f\"Features: {feat_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64110761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "setosa        50\n",
       "versicolor    50\n",
       "virginica     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Three class labels\n",
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ddf6a",
   "metadata": {},
   "source": [
    "#### 1.1 Create Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d332bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 5), (30, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 42 # this is the answer. But what is the question?\n",
    "df_random = df.sample(frac=1, random_state=random_state)\n",
    "\n",
    "dftrain = df_random[0:120]\n",
    "dftest = df_random[120:]\n",
    "dftrain.shape, dftest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90116ea1",
   "metadata": {},
   "source": [
    "## 2. Define helper functions <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa306f",
   "metadata": {},
   "source": [
    "#### 2.1 Gini Inpurity\n",
    "\n",
    "Suppose we have a vector $L$ of labels (integers or chars), \n",
    "we can count the occurence of each label (get their frequency). <br>\n",
    "Subsequently, the probability for each label $x$ is defined as $P(x_i)$\n",
    "then the GINI inpurity is defined as: <br>\n",
    "    $$GINI(x) = 1 - \\sum_i^N (P(x_i))^2$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb18c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(x):\n",
    "    unq, counts = np.unique(x, return_counts=True)\n",
    "    probs = counts / len(x)\n",
    "    gini = 1 - (probs **2 ).sum()\n",
    "    return gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d9abd",
   "metadata": {},
   "source": [
    "#### 2.2 Entropy\n",
    "\n",
    "Suppose we have a vector $L$ of labels (integers or chars), \n",
    "we can count the occurence of each label (get their frequency). <br>\n",
    "Subsequently, the probability for each label $x$ is defined as $P(x_i)$\n",
    "then the Entropy is defined as: \n",
    "    $$Entropy(x) = \\sum_i^N -P(x_i)\\log_2 (P(x_i))$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e7aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    unq, counts = np.unique(x, return_counts=True)\n",
    "    probs = counts/len(x)\n",
    "    entropy = -probs * np.log2(probs)\n",
    "    entropy = entropy.sum()\n",
    "    return entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d052393",
   "metadata": {},
   "source": [
    "#### 2.3 Information Gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e8402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(feats: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     value: float,\n",
    "                     metric='gini'):\n",
    "    assert len(feats) == len(labels)  , f\"len(feats): {len(feats)} | len(labels): {len(labels)}\"  \n",
    "    f = gini_impurity if metric == \"gini\" else entropy\n",
    "    left_mask = feats <= value\n",
    "    left_y = labels[left_mask]\n",
    "    right_y = labels[~left_mask]\n",
    "    \n",
    "    left_wt = len(left_y) / len(labels)\n",
    "    right_wt = len(right_y) / len(labels)\n",
    "    \n",
    "    gain = f(labels) - ( left_wt * f(left_y) +  right_wt * f(right_y) )\n",
    "    return gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bc8ce",
   "metadata": {},
   "source": [
    "## 3. Combine them in a class <a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a552448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # class-level variable to automatically assign the ID of each node\n",
    "    ID_CTR = 0\n",
    "    def __init__(self):\n",
    "        self.id = Node.ID_CTR\n",
    "        Node.ID_CTR+=1\n",
    "        self.level = -1\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        \n",
    "        ## If its a leaf node\n",
    "        self.isLeaf = True\n",
    "        self.leaf_label = None\n",
    "        \n",
    "        # Children\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    @classmethod\n",
    "    def make_split_node(cls, level, split_feature, split_value):\n",
    "        # Split node contain the information about the feature F and the value of F: V, which is used to split\n",
    "        node = Node()\n",
    "        node.level = level\n",
    "        node.split_feature = split_feature\n",
    "        node.split_value = split_value\n",
    "        node.isLeaf = False\n",
    "        return node\n",
    "    \n",
    "    @classmethod\n",
    "    def make_leaf_node(cls, level, label):\n",
    "        # Leaf node only contain the information of the label\n",
    "        node = Node()\n",
    "        node.isLeaf = True\n",
    "        node.level = level\n",
    "        node.leaf_label = label\n",
    "        return node\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f\"L: {self.level}\"\n",
    "        if self.isLeaf:\n",
    "            s+= f\"| Label: {self.leaf_label}\"\n",
    "            return s\n",
    "        \n",
    "        s += f\"| Feat: {self.split_feature}\"\n",
    "        s += f\"| Thr: {self.split_value}\"\n",
    "        return s\n",
    "        \n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth: int):\n",
    "        self.root: Node = None\n",
    "        self.max_depth: int = max_depth\n",
    "        from treelib import Tree, Node\n",
    "        self.prtree = Tree()\n",
    "        \n",
    "    def fit_tree(self, X, Y):\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        assert len(X) == len(Y), f\"Labels and Data sizes are not same: X:{len(X)}, Y: {len(Y)}\"\n",
    "        self.root = self._build_tree(X, Y, 0)\n",
    "        \n",
    "    def _build_tree(self, X, Y, level):\n",
    "        \"\"\"\n",
    "        1. Check if termination is reached because all labels are same \n",
    "           or max depth is reached\n",
    "        2. If Max Depth is reached, then we return the label as the max count in the mixture.\n",
    "        \"\"\"\n",
    "        if level == self.max_depth or len(np.unique(Y)) == 1:\n",
    "            # terminate\n",
    "            if len(np.unique(Y)) == 1: # If there is only one type of samples in the split.\n",
    "                node = Node.make_leaf_node(level, np.unique(Y)[0])\n",
    "            elif len(np.unique(Y)) > 1: # We have max depth, so we will return the majority sample in this split\n",
    "                labels, counts = np.unique(Y, return_counts=True)\n",
    "                node = Node.make_leaf_node(level, labels[counts.argmax()])\n",
    "            else:\n",
    "                print(\"failure: \", level, Y)\n",
    "            \n",
    "            return node\n",
    "            \n",
    "        best_split = self._find_best_split(X, Y)\n",
    "        root = Node.make_split_node(level, \n",
    "                                    best_split[\"feature_id\"], \n",
    "                                    best_split[\"value\"])\n",
    "        \n",
    "        \n",
    "        f = best_split[\"feature_id\"]\n",
    "        v = best_split[\"value\"]\n",
    "        left_mask = X[:, f] <= v\n",
    "        if left_mask.sum() > 0:\n",
    "            root.left = self._build_tree(X[left_mask], Y[left_mask], level + 1)\n",
    "        if (~left_mask).sum() > 0:\n",
    "            root.right = self._build_tree(X[~left_mask], Y[~left_mask], level + 1)\n",
    "        return root\n",
    "        \n",
    "    def _find_best_split(self, X, Y):\n",
    "        \"\"\"\n",
    "        Goes through each value in each feature and computes the gain. \n",
    "        Returns the best split corresponding to max gain\n",
    "        \"\"\"\n",
    "        max_gain = float('-inf')\n",
    "        best_split = {}\n",
    "\n",
    "        for f in range(X.shape[1]):\n",
    "            feats = X[:, f]\n",
    "            unq_values = np.unique(feats)\n",
    "            for value in unq_values:\n",
    "                gain = information_gain(feats, Y, value, metric=\"gini\")\n",
    "                if gain > max_gain:\n",
    "                    best_split[\"feature_id\"] = f\n",
    "                    best_split[\"value\"] = value\n",
    "                    best_split[\"gain\"] = gain\n",
    "                    max_gain = gain\n",
    "        return best_split\n",
    "    \n",
    "    def traverse_tree(self, X):\n",
    "        cur = self.root\n",
    "        while cur.isLeaf is False:\n",
    "            if X[cur.split_feature] <= cur.split_value:\n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.leaf_label\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for row in X:\n",
    "            preds.append(self.traverse_tree(row))\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    def _print_tree_helper(self, root, parent):\n",
    "        if root is None:\n",
    "            return\n",
    "        self.prtree.create_node(str(root), root, parent=parent)\n",
    "        if root.isLeaf:\n",
    "            return\n",
    "        self._print_tree_helper(root.left, root)\n",
    "        self._print_tree_helper(root.right, root)\n",
    "        \n",
    "            \n",
    "    def print_tree(self):\n",
    "        root = self.root\n",
    "        self.prtree.create_node(\"ROOT\", \"root\")\n",
    "        self._print_tree_helper(self.root, \"root\")\n",
    "        self.prtree.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d4515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTree(3)\n",
    "dtree.fit_tree(dftrain.drop(\"species\", axis=1).to_numpy(), \n",
    "               dftrain[\"species\"].to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16be250-2a6d-40b6-ac2f-129159579148",
   "metadata": {},
   "source": [
    "## 3.1 Visualization of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c4be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "└── L: 0| Feat: 2| Thr: 1.9\n",
      "    ├── L: 1| Feat: 3| Thr: 1.7\n",
      "    │   ├── L: 2| Feat: 2| Thr: 4.8\n",
      "    │   │   ├── L: 3| Label: virginica\n",
      "    │   │   └── L: 3| Label: virginica\n",
      "    │   └── L: 2| Feat: 2| Thr: 4.9\n",
      "    │       ├── L: 3| Label: versicolor\n",
      "    │       └── L: 3| Label: virginica\n",
      "    └── L: 1| Label: setosa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0423569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "xtest = dftest.drop(\"species\", axis=1).to_numpy()\n",
    "print(xtest.shape)\n",
    "preds = dtree.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a49170-ff76-43d4-bbd6-de729020e13a",
   "metadata": {},
   "source": [
    "## 3.2 Finding accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc667576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 96.67%\n"
     ]
    }
   ],
   "source": [
    "gt = dftest.species\n",
    "TP = preds == gt\n",
    "accuracy = TP.to_numpy().sum() / len(xtest)\n",
    "print(f\"Accuracy on the test set: {100*np.round(accuracy, 4)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e423e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
