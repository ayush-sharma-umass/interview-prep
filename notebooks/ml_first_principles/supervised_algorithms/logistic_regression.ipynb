{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ef0912-e7b7-4971-8e4d-eeb3bad1df2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60903e00-b063-4053-9940-db31f3fc7271",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "* [What is Logistic Regression](#1)\n",
    "* [Overview of the algorithm](#2)\n",
    "* [A word on the Logistic function](#3)\n",
    "* [Logistic Regression is effective when](#4)\n",
    "* [When to watch out for?](#5)\n",
    "* [Code](#6)\n",
    "* [Metric used to evaluate](#7)\n",
    "* [Interview questions](#8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae52f6e-308b-48c7-b039-bc8524c0d12e",
   "metadata": {},
   "source": [
    "# What is Logistic regression? <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "Logistic regression is a statistical modeling technique used to analyze the relationship between a dependent variable and one or more independent variables, where the dependent variable represents a binary outcome or a categorical outcome with multiple classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d405c8-b4e9-42ed-b07c-6156995d9ac4",
   "metadata": {},
   "source": [
    "# Overview of the algorithm <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "* *Data processing:* <br>\n",
    "    Since the logistic regression is sensitive to the scales of the features of the input data, we need to normalize the data. If we don't normalize the data, some input variables with higher scales will end up having uneven influence on the model.\n",
    "* *Removing outliers* <br>\n",
    "    Unlike decision trees, logistic regression is sensitive to outliers. Thus, it is important to visualize the data and remove the outliers.\n",
    "* *Linear combination of independent variables:* <br>\n",
    "    Logistic regression creates a linear combination of input variables by multiplying them to the coefficients of the weights. The results are then summed up.\n",
    "* *Sigmoid function:* <br>\n",
    "    The linear combination output is then non-linearly mapped to a value between $[0,1]$ representing the estimated probability of the outcome.\n",
    "* *Gradient Descent or Maximum Likelihood Estimation:* <br>\n",
    "    The coefficients for the independent variables which we called weights are estimated using **Gradient Descent** or **Maximum likelihood estimation**. We want to find the parameters which maximize the probability of occurence of the data given its predictions.\n",
    "* *Evaluation:* <br>\n",
    "    The model can be evaluaed using several metrics such as acuracy or precision & recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded4f5d-8520-45fb-b45d-218b68424349",
   "metadata": {},
   "source": [
    "# A word on the Logistic function <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "**Logistic function:** \n",
    "\n",
    "$$ f(x)= {L \\over (1+e^{-k(xâˆ’x0)}) }$$ \n",
    "\n",
    "In a logistic fundtion : <br>\n",
    " * $L$ controls the max value the function can take\n",
    " * $k$ controls the steepness of the curve\n",
    " * $x_0$ controls where the function is centered on the x axis\n",
    "\n",
    "**Sigmoid function:**\n",
    "\n",
    "$$ S(x) = { 1 \\over (1 + e^{-x}) }$$\n",
    "\n",
    "Sigmoid function is a special case of logistic function where, \n",
    " * max value the function can take is $1$\n",
    " * $x_0 = 0$ i.e. the function is centered at 0\n",
    " * The steepnes of the curve is $1$\n",
    "\n",
    "The Sigmoid function takes an unbounded real value and maps it to a real value between 0 and 1. <br>\n",
    "\n",
    "**But what makes it a good choice for modeling probabilities? <br>**\n",
    " * We can map the linear combination of input variables to a value to $[0, 1]$.\n",
    " * It is a continous function.\n",
    " * It is non linear\n",
    " * It is monotonously increasing.\n",
    " * It is a differentiable function.\n",
    " \n",
    "**What are some limitations of the sigmoid function?**\n",
    " * It gives rise to vanishing gradients: when the x values are large in magnitude (towards the extremeties), it tends to produce a very small change in the Y value. Hence, the gradients produced are near 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d567cb-8923-4374-9226-7d2531e6f042",
   "metadata": {},
   "source": [
    "# Code: Binary Logistic Regression\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d26852-0e06-4ff5-8e12-0a638d99714a",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93e048e-57e4-438c-8257-fdf1220a4eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c716039-a683-4206-a79c-093aa3f3a34a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "* Note that in this code, we use **Xavier initialization** for initialize our weights. We can also use normal initialization for this. You can run it with either and see the effect on the process. Random initialization converges slower and yields less accuracy as compared to Xavier initialization. <br>\n",
    " Checkout the function: `xavier_init`\n",
    " * We use gradient descent here to optimize: <br>\n",
    "   Let $X$ be our input data consistting of $N$ samples, $W$ be our weights and $b$ be the bias, and $Y_T$ be the ground truth. <br>\n",
    "   $$ H = W.X + b $$ <br>\n",
    "   $$ Y_{pred} = Sigmoid(H) $$  <br>\n",
    "   $$ BCE_{loss} = L = -{\\sum_{i=1}^N( Y_T{log}(Y_pred) + (1 - Y_T) log (1 - Y_pred) ) \\over N }$$ <br>\n",
    "   So, now when we compute the gradient wrt loss, we get, \n",
    "   $$ {{\\partial L} \\over {\\partial Y_{pred}}}  = { {-1\\over N}[ {Y_T \\over Y_pred} - {(1-Y_T) \\over (1-Y_pred)} ] }$$\n",
    "   Partial gradient of predictions over sigmoid output<br>\n",
    "   (check stack overflow or derive yourself, its easy takes a few equation manipulations)\n",
    "   $$ {\\partial Y_{pred} \\over \\partial H}  = H * (1 - H)$$ <br>\n",
    "   Partial gradient of $H$ over weights $W$\n",
    "   $$ {\\partial H \\over \\partial W} = X $$\n",
    "   Partial gradient of $H$ over bias $b$\n",
    "   $$ {\\partial H \\over \\partial b} = 1 $$\n",
    "   \n",
    "   Now, using **chain-rule** of derivatives, we can stitch to find the gradient of loss $L$ wrt weights $W$ and bias $W$,\n",
    "   \n",
    "   $$ {\\partial H \\over \\partial W} = {\\partial L \\over \\partial Y_{pred}} {\\partial Y_{pred} \\over \\partial H} {\\partial H \\over \\partial W} $$\n",
    "   \n",
    "   Similary for bias $b$, \n",
    "\n",
    "   $$ {\\partial H \\over \\partial b} = {\\partial L \\over \\partial Y_{pred}} {\\partial Y_{pred} \\over \\partial H} {\\partial H \\over \\partial b} $$\n",
    "   \n",
    " Checkout the functions `_forward_pass` and `_update_weights` on how these equations are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "763dca68-3694-40e0-b5b8-bca1c2b04a5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr=1e-4, niter=5000, lambda_reg=0.0):\n",
    "        self.lr = lr\n",
    "        self.niter = niter\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_features = None\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.eps = 1e-6\n",
    "        self.loss_history = None\n",
    "        \n",
    "    def fit(self, X, Y, init_wt='random'):\n",
    "        self.nsamples, self.nfeatures = X.shape\n",
    "        if init_wt == 'random':\n",
    "            self.weights = np.random.rand(self.nfeatures)\n",
    "        elif init_wt == 'xavier':\n",
    "            self.weights = self.xavier_init(X.shape, (self.nfeatures, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid initialization given for weights\")\n",
    "\n",
    "        self.weights = self.weights.flatten()\n",
    "        self.bias = 0\n",
    "        self.loss_history = []\n",
    "        X = self._normalize_data(X)\n",
    "        for i in range(self.niter):\n",
    "            ypred = self._forward_pass(X)\n",
    "            loss = self._bceloss(ypred, Y)\n",
    "            self.loss_history.append(loss)\n",
    "            if i % 500 == 0:   \n",
    "                print(f\"iter: {i+1} --> Loss: {loss}\")\n",
    "            self._update_weights(X, Y, ypred)\n",
    "       \n",
    "    \n",
    "    def predict(self, Xtest, Ytest):\n",
    "        Xtest = self._normalize_data(Xtest)\n",
    "        Ypred = self._forward_pass(Xtest)\n",
    "        th = 0.5\n",
    "        prP = Ypred > th \n",
    "        prN = Ypred <= th\n",
    "        gtP = Ytest == 1\n",
    "        gtN = Ytest == 0\n",
    "        tp = np.sum(prP & gtP)\n",
    "        tn = np.sum(prN & gtN)\n",
    "        fp = np.sum(prP & gtN)\n",
    "        fn = np.sum(prN & gtP)\n",
    "        precision = tp/(tp + fp)\n",
    "        recall = tp/(tp + fn)\n",
    "        accuracy = tp/len(Xtest)\n",
    "        print(f\"Precision = {precision}\")\n",
    "        print(f\"recall = {recall}\")\n",
    "        return Ypred\n",
    "    \n",
    "    def xavier_init(self, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        Xavier initialization for weights in a linear layer\n",
    "        :param input_shape: tuple, shape of input to layer\n",
    "        :param output_shape: tuple, shape of output from layer\n",
    "        :return: ndarray, initialized weights for layer\n",
    "        \"\"\"\n",
    "        scale = np.sqrt(2 / (input_shape[0] + output_shape[0]))\n",
    "        return np.random.randn(input_shape[1], output_shape[1]) * scale\n",
    "    \n",
    "    \n",
    "    def _normalize_data(self, X):\n",
    "        \"\"\"\n",
    "        Normalizes the data by subtracing the mean and dividing by std.\n",
    "        Note that this doesn't fix the range between -1 to 1. \n",
    "        X: data with shape: (num_samples, num_feats)\n",
    "        \"\"\"\n",
    "        mu = np.mean(X, axis=0, keepdims=True)\n",
    "        std = np.std(X, axis=0, keepdims=True)\n",
    "        Xnew = (X - mu) / (std + self.eps)\n",
    "        return Xnew\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _bceloss(self, ypred, ytrue):\n",
    "        \"\"\"\n",
    "        BInary cross entropy loss\n",
    "        \"\"\"\n",
    "        assert len(ypred) == len(ytrue) and len(ypred) > 0\n",
    "        nsamples = len(ypred)\n",
    "        loss = (-1./nsamples) * np.sum( (ytrue)* np.log(ypred + self.eps) + (1. - ytrue) * np.log(1 - ypred + self.eps) )\n",
    "        reg = (self.lambda_reg/2) * np.sum(self.weights ** 2)\n",
    "        return loss + reg\n",
    "                                 \n",
    "    \n",
    "    def _forward_pass(self, X):\n",
    "        H = np.dot(X, self.weights) + self.bias\n",
    "        # Clip predicted probabilities to prevent overflow\n",
    "        ypred = np.clip(self._sigmoid(H), self.eps, 1 - self.eps)\n",
    "        return ypred\n",
    "        \n",
    "    \n",
    "    def _update_weights(self, X, Yt, Yp):\n",
    "        # dLoss/dy = -1/N[ Yt/Yp - (1-Yt)/(1-Yp) ]\n",
    "        # dY/dH = H*(1-H)\n",
    "        # dH/dW = X\n",
    "        # dH/dB = 1\n",
    "        nsamples = len(X)\n",
    "        dLoss_dy = (-1./nsamples) * np.sum( Yt/(Yp + self.eps) - (1 - Yt)/(1 - Yp + self.eps) )\n",
    "        dy_dH = Yp * (1 - Yp)\n",
    "        dH_dW = X\n",
    "        dW = np.dot(dLoss_dy * dy_dH, dH_dW) + self.lambda_reg * self.weights # mine\n",
    "        dB = np.mean(dLoss_dy * dy_dH * 1.)\n",
    "        self.weights = self.weights - self.lr * dW\n",
    "        self.bias = self.bias - self.lr * dB\n",
    "        \n",
    "    \n",
    "    def _sigmoid(self, X):\n",
    "        X = np.clip(X, -500, 500) # we clip the input to \n",
    "        return 1. / (1. + np.exp(-X))\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e0f94ab-69d3-4e44-a972-fd23b4049f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c71918cb-f580-44fd-aff7-2b8a69e7b2f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 --> Loss: 0.053960030612626136\n",
      "iter: 501 --> Loss: 2275.1495605406435\n",
      "iter: 1001 --> Loss: 1378.1400801191146\n",
      "iter: 1501 --> Loss: 834.1443545827769\n",
      "iter: 2001 --> Loss: 505.895954097304\n",
      "iter: 2501 --> Loss: 320.00074065801334\n",
      "iter: 3001 --> Loss: 231.90421043888463\n",
      "iter: 3501 --> Loss: 189.68302851950628\n",
      "iter: 4001 --> Loss: 166.26852639179222\n",
      "iter: 4501 --> Loss: 153.2239452213424\n",
      "iter: 5001 --> Loss: 143.8548887025751\n",
      "iter: 5501 --> Loss: 135.53265853514893\n",
      "iter: 6001 --> Loss: 127.76577939009245\n",
      "iter: 6501 --> Loss: 120.43877531490958\n",
      "iter: 7001 --> Loss: 113.50396349967983\n",
      "iter: 7501 --> Loss: 106.93316949102513\n",
      "iter: 8001 --> Loss: 100.70532935650601\n",
      "iter: 8501 --> Loss: 94.80237782496042\n",
      "iter: 9001 --> Loss: 89.20780302861064\n",
      "iter: 9501 --> Loss: 83.90976880615658\n"
     ]
    }
   ],
   "source": [
    "blr = BinaryLogisticRegression(lr=1e-3, niter=10000, lambda_reg=0.5)\n",
    "blr.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70e53371-1fe8-4165-92f8-75a599e989a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.0\n",
      "recall = 1.0\n"
     ]
    }
   ],
   "source": [
    "Ypred = blr.predict(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf469400-e23c-466b-9213-1f3181053272",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99932852e-01, 1.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99951860e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 1.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.91267379e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99529572e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99980219e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99883912e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99999000e-01, 1.00000000e+00],\n",
       "       [9.99656716e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99734147e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00],\n",
       "       [1.00000000e-06, 0.00000000e+00],\n",
       "       [9.99984054e-01, 1.00000000e+00],\n",
       "       [9.99999000e-01, 2.00000000e+00]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([Ypred, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d6ee1-69e4-4a1c-ac40-7782949b0b1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Why is initialization of weights important? <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "Initialization of the weights or coefficients is crucial as it sets the starting values for the optimization algorithm used to estimate the model parameters. Proper initialization helps ensure the convergence of the optimization process and the stability of the estimated coefficients. \n",
    "\n",
    "**What happens if weights are not initialized properly?**\n",
    "\n",
    "1. *Slow /Poor convergence:* <br>\n",
    "   Poor initialization may lead to slow convergence or the algorithm getting stuck in local optima. \n",
    "2. *Degeneracy*: <br>\n",
    "    If the model fails to converge, we may get unreliable results. This is called degeneracy. This can happen especially especially when the input features of the data are highly correlated.\n",
    "4. *Stability issues:* <br>\n",
    "   Well-initialized weights ensure that the optimization algorithm explores a reasonable parameter space, avoiding extreme or unrealistic estimates.If the weights are not initialized properly, model may explore the regions which have poor interpretability and may yield non-sensical results.\n",
    "4. *Influence on regularization:* <br>\n",
    "   If the weights are not initialized, then an $L1$ or $L2$ which depends on the weights can cause undesirable effects on the weights update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544af47-4f86-4d41-a848-7c1b5e8e1c0b",
   "metadata": {},
   "source": [
    "# Code: Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0acfbe-781d-4654-834d-637590d9d566",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8f4744-a23b-45c0-abf5-139b3fca8a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootpath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fadc0b5e-0fe2-45e2-88d6-f936be61f082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 150\n",
      "Total features: 5\n",
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n"
     ]
    }
   ],
   "source": [
    "csvpath = os.path.join(rootpath, \"assets/iris.csv\")\n",
    "df = pd.read_csv(csvpath)\n",
    "num_samples, num_feats = df.shape\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "print(f\"Total features: {num_feats}\")\n",
    "feat_names = list(df)\n",
    "print(f\"Features: {feat_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e75650a8-6113-468d-8f54-e851cbb0c964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "setosa        50\n",
       "versicolor    50\n",
       "virginica     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the frequency\n",
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faedfe2c-1801-4d5d-9f07-a7e49e70a3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a unique string-->int label map\n",
    "\n",
    "\n",
    "unique_labels = np.unique(df.species.to_numpy())\n",
    "label_dict = {label: index for index, label in enumerate(unique_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6acce-082e-4a62-a7b2-ec470bc891d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
